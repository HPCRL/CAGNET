#!/bin/bash
#BSUB -P BIF115
#BSUB -W 0:10
#BSUB -nnodes 1
#BSUB -alloc_flags gpumps
#BSUB -J distr-gnn
#BSUB -o distr-gnn.%J
#BSUB -e distr-gnn.%J

conda activate gnn

module load cuda
module load gcc

module list

hostarr=(${LSB_HOSTS})
# echo $LSB_MCPU_HOSTS
# hostarr=(${LSB_MCPU_HOSTS})
echo ${hostarr[2]}
ddlrun -x WORLD_SIZE=4 -x MASTER_ADDR=${hostarr[2]} -x MASTER_PORT=1234 -accelerators 4 python gcn_distr_2d_gpu.py --accperrank=4 --epochs=1 --graphname=Cora --timing=True --midlayer=16
# /sw/summit/xalt/1.2.0/bin/mpirun -x WORLD_SIZE=4 -x MASTER_ADDR=${hostarr[2]} -x MASTER_PORT=1234 -x LD_LIBRARY_PATH -x LSB_MCPU_HOSTS -x LSB_JOBID -x PATH -x PYTHONPATH -mca plm_rsh_num_concurrent 1 --rankfile /tmp/DDLRUN/DDLRUN.fCE00UxmRBJf/RANKFILE -n 4 -x DDL_HOST_PORT=2200 -x "DDL_HOST_LIST=${hostarr[2]}:0,1,2,3" python gcn_distr_2d_gpu_run.py --accperrank=4

# #!/bin/bash
# #BSUB -P BIF115
# #BSUB -W 0:10
# #BSUB -nnodes 4
# #BSUB -alloc_flags gpumps
# #BSUB -J distr-gnn
# #BSUB -o distr-gnn.%J
# #BSUB -e distr-gnn.%J
# 
# conda activate gnn
# 
# hostarr=(${LSB_HOSTS})
# # echo $LSB_MCPU_HOSTS
# # hostarr=(${LSB_MCPU_HOSTS})
# echo ${hostarr[2]}
# ddlrun -x WORLD_SIZE=16 -x MASTER_ADDR=${hostarr[2]} -x MASTER_PORT=1234 -accelerators 4 python gcn_distr_2d_gpu.py --accperrank=4
